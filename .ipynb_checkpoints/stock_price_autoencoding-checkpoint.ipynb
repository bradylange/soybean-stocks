{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Developer: Brady Lange\n",
    "# 11/08/2019\n",
    "# Description:\n",
    "\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, UpSampling1D, BatchNormalization, LSTM, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "from keras import regularizers\n",
    "import datetime\n",
    "import time\n",
    "import requests as req\n",
    "import json\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# startdate = sbean_cont_mar[\"Date\"][0]\n",
    "# startdate = sbean_cont_may[\"Date\"][0]\n",
    "startdate = sbean_cont_july[\"Date\"][0]\n",
    "window_length = 10\n",
    "encoding_dim = 3\n",
    "epochs = 100\n",
    "test_samples = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdate(ts):\n",
    "    return datetime.datetime.fromtimestamp(\n",
    "        int(ts)\n",
    "    ).strftime('%Y-%M-%d')\n",
    "\n",
    "def plot_examples(stock_input, stock_decoded):\n",
    "    n = 10  \n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i, idx in enumerate(list(np.arange(0, test_samples, 200))):\n",
    "        # display original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(\"Input\", fontweight=600)\n",
    "        else:\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "        plt.plot(stock_input[idx])\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        \n",
    "\n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(\"Output\", fontweight=600)\n",
    "        else:\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "        plt.plot(stock_decoded[idx])\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        \n",
    "        \n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.title(\"Train loss\")\n",
    "    ax = plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.title(\"Test loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CONVERT DATES TO STRINGS TO BE CONVERTED TO TIMESTAMPS\n",
    "# TODO: SEPERATE DATASETS (THEY'RE NOT THE SAME!)\n",
    "\n",
    "# Load active contracts for March dataset\n",
    "sbean_cont_mar = pd.read_excel(r\"./data/active_soybean_contracts_for_march_2020.xlsx\", sheet_name = \"ZS_H_2020.CSV\", skiprows = 3)\n",
    "# Load active contracts for May dataset\n",
    "sbean_cont_may = pd.read_excel(r\"./data/active_soybean_contracts_for_may_2020.xlsx\", sheet_name = \"ZS_K_2020.CSV\", skiprows = 3)\n",
    "# Load active contracts for July dataset\n",
    "sbean_cont_july = pd.read_excel(r\"./data/active_soybean_contracts_for_july_2020.xlsx\", sheet_name = \"ZS_N_2020.CSV\", skiprows = 3)\n",
    "\n",
    "\n",
    "sbean_cont_mar.sort_values(by = \"Date\", inplace = True)\n",
    "sbean_cont_may.sort_values(by = \"Date\", inplace = True)\n",
    "sbean_cont_july.sort_values(by = \"Date\", inplace = True)\n",
    "\n",
    "# sbean_cont_all.insert(0, \"Year\", sbean_cont_all[\"Date\"].dt.year)\n",
    "# sbean_cont_all.insert(1, \"Month\", sbean_cont_all[\"Date\"].dt.month)\n",
    "# sbean_cont_all.insert(2, \"Day\", sbean_cont_all[\"Date\"].dt.day)\n",
    "# sbean_cont_all.insert(3, \"Avg_Price\", \n",
    "#                       (sbean_cont_all[\"Open\"] + sbean_cont_all[\"High\"] \n",
    "#                       + sbean_cont_all[\"Low\"] + sbean_cont_all[\"Close\"]) \n",
    "#                       / sbean_cont_all[[\"Open\", \"High\", \"Low\", \"Close\"]].shape[1])\n",
    "sbean_cont_mar.drop([\"Open\", \"High\", \"Low\"], axis = 1, inplace = True)\n",
    "sbean_cont_may.drop([\"Open\", \"High\", \"Low\"], axis = 1, inplace = True)\n",
    "sbean_cont_july.drop([\"Open\", \"High\", \"Low\"], axis = 1, inplace = True)\n",
    "\n",
    "#sbean_cont_all[[\"Year\", \"Month\", \"Day\"]] = sbean_cont_all[[\"Year\", \"Month\", \"Day\"]].astype(str)\n",
    "\n",
    "sbean_cont_mar[\"Date\"] = sbean_cont_mar[\"Date\"].astype(str)\n",
    "sbean_cont_may[\"Date\"] = sbean_cont_may[\"Date\"].astype(str)\n",
    "sbean_cont_july[\"Date\"] = sbean_cont_july[\"Date\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets retrieval & transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "start_timestamp = time.mktime(datetime.datetime.strptime(startdate, \"%Y-%M-%d\").timetuple())\n",
    "end_timestamp = time.mktime(datetime.datetime.strptime(max(sbean_cont_mar[\"Date\"]), \"%Y-%M-%d\").timetuple())\n",
    "\n",
    "days = []\n",
    "for day in sbean_cont_mar[\"Date\"]:\n",
    "    days.append(time.mktime(datetime.datetime.strptime(day, \"%Y-%M-%d\").timetuple()))\n",
    "sbean_cont_mar.insert(0, \"Timestamp\", days)\n",
    "sbean_cont_mar.drop([\"Date\"], axis = 1, inplace = True)\n",
    "\n",
    "sbean_cont_mar = sbean_cont_mar.to_numpy().tolist()\n",
    "\n",
    "one_week = 3600*24*7 # s\n",
    "one_day = 3600*24 # s\n",
    "weeks = list(np.arange(start_timestamp, end_timestamp, one_week))\n",
    "days_recorded = (datetime.datetime.fromtimestamp(end_timestamp)-datetime.datetime.fromtimestamp(start_timestamp)).days\n",
    "print(\"days_recorded \", days_recorded)\n",
    "data = []\n",
    "if not os.path.isfile(\"data.pickle\"):\n",
    "    for i in range(1, len(weeks)):\n",
    "        start_weekday = mkdate(weeks[i-1])\n",
    "        end_weekday = mkdate(weeks[i]-one_day)\n",
    "        print(start_weekday, end_weekday)\n",
    "        sbean_cont_mar.sort(key=lambda x: x[0])\n",
    "        for pricepoint in sbean_cont_mar:\n",
    "            if pricepoint[0] >= weeks[i-1] and pricepoint[0] < (weeks[i]-one_day):\n",
    "                data.append([int(pricepoint[0]), pricepoint[1]])\n",
    "                \n",
    "    pickle.dump(data, open(\"./data.pickle\", \"wb\"))\n",
    "else:\n",
    "    data = pickle.load(open(\"./data.pickle\", \"rb\"))\n",
    "\n",
    "df = pd.DataFrame(np.array(data)[:,1], columns=['price'])\n",
    "df['pct_change'] = df.price.pct_change()\n",
    "df['log_ret'] = np.log(df.price) - np.log(df.price.shift(1))\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_train_nonscaled = np.array([df['log_ret'].values[i-window_length:i].reshape(-1, 1) for i in tqdm(range(window_length+1,len(df['log_ret'])))])\n",
    "x_train = np.array([scaler.fit_transform(df['log_ret'].values[i-window_length:i].reshape(-1, 1)) for i in tqdm(range(window_length+1,len(df['log_ret'])))])\n",
    "\n",
    "x_test = x_train[-test_samples:]\n",
    "x_train = x_train[:-test_samples]\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(np.array(data)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of test data: {}%\".format((test_samples/len(x_train))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple feed-forward autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_simple = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test_simple = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "# this is our input placeholder\n",
    "input_window = Input(shape=(window_length,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_window)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(window_length, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_window, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_window, encoded)\n",
    "\n",
    "\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = autoencoder.fit(x_train_simple, x_train_simple,\n",
    "                epochs=epochs,\n",
    "                batch_size=1024,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_simple, x_test_simple))\n",
    "\n",
    "decoded_stocks = autoencoder.predict(x_test_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)\n",
    "plot_examples(x_test_simple, decoded_stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_deep = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test_deep = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "input_window = Input(shape=(window_length,))\n",
    "\n",
    "x = Dense(6, activation='relu')(input_window)\n",
    "x = BatchNormalization()(x)\n",
    "encoded = Dense(encoding_dim, activation='relu')(x)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "\n",
    "x = Dense(6, activation='relu')(encoded)\n",
    "x = BatchNormalization()(x)\n",
    "decoded = Dense(window_length, activation='sigmoid')(x)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_window, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_window, encoded)\n",
    "\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = autoencoder.fit(x_train_deep, x_train_deep,\n",
    "                epochs=epochs,\n",
    "                batch_size=1024,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_deep, x_test_deep))\n",
    "\n",
    "decoded_stocks = autoencoder.predict(x_test_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "orig = scaler.inverse_transform(x_test.reshape(100, 10))\n",
    "# orig = exp(orig)\n",
    "# min_x_test = np.min(orig)\n",
    "# max_x_test = np.max(orig)\n",
    "\n",
    "dec = scaler.inverse_transform(decoded_stocks)\n",
    "# dec = exp(dec)\n",
    "\n",
    "# Relative root mean squared error\n",
    "sqrt(mean_squared_error(orig, dec)) #/ (max_x_test - min_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_examples(x_test_deep, decoded_stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Convolutional autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_window = Input(shape=(window_length,1))\n",
    "x = Conv1D(16, 3, activation=\"relu\", padding=\"same\")(input_window) # 10 dims\n",
    "#x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(2, padding=\"same\")(x) # 5 dims\n",
    "x = Conv1D(1, 3, activation=\"relu\", padding=\"same\")(x) # 5 dims\n",
    "#x = BatchNormalization()(x)\n",
    "encoded = MaxPooling1D(2, padding=\"same\")(x) # 3 dims\n",
    "\n",
    "encoder = Model(input_window, encoded)\n",
    "\n",
    "# 3 dimensions in the encoded layer\n",
    "\n",
    "x = Conv1D(1, 3, activation=\"relu\", padding=\"same\")(encoded) # 3 dims\n",
    "#x = BatchNormalization()(x)\n",
    "x = UpSampling1D(2)(x) # 6 dims\n",
    "x = Conv1D(16, 2, activation='relu')(x) # 5 dims\n",
    "#x = BatchNormalization()(x)\n",
    "x = UpSampling1D(2)(x) # 10 dims\n",
    "decoded = Conv1D(1, 3, activation='sigmoid', padding='same')(x) # 10 dims\n",
    "autoencoder = Model(input_window, decoded)\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = autoencoder.fit(x_train, x_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=1024,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n",
    "\n",
    "decoded_stocks = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_examples(x_test_deep, decoded_stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (recurrent neural networks) autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(window_length, 1))\n",
    "encoded = LSTM(encoding_dim)(inputs)\n",
    "\n",
    "decoded = RepeatVector(window_length)(encoded)\n",
    "decoded = LSTM(1, return_sequences=True)(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)\n",
    "encoder = Model(inputs, encoded)\n",
    "sequence_autoencoder.summary()\n",
    "\n",
    "sequence_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = sequence_autoencoder.fit(x_train, x_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=1024,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n",
    "\n",
    "decoded_stocks = sequence_autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_examples(x_test, decoded_stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple AE + augmention with synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesized = []\n",
    "required_nums = [0, 1]\n",
    "optional_nums = list(np.arange(0.1, 0.9, 0.1))\n",
    "for i in tqdm(range(100000)):\n",
    "    combo = list(np.random.choice(optional_nums, 8))+required_nums\n",
    "    np.random.shuffle(combo)\n",
    "    synthesized.append(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_simple = np.concatenate((x_train.reshape((len(x_train), np.prod(x_train.shape[1:]))),synthesized))\n",
    "x_test_simple = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "\n",
    "input_window = Input(shape=(window_length,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_window)\n",
    "decoded = Dense(window_length, activation='sigmoid')(encoded)\n",
    "autoencoder = Model(input_window, decoded)\n",
    "encoder = Model(input_window, encoded)\n",
    "\n",
    "\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = autoencoder.fit(x_train_simple, x_train_simple,\n",
    "                epochs=epochs,\n",
    "                batch_size=1024,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_simple, x_test_simple))\n",
    "\n",
    "decoded_stocks = autoencoder.predict(x_test_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_examples(x_test_simple, decoded_stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep autoencoder + synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_deep = np.concatenate((x_train.reshape((len(x_train), np.prod(x_train.shape[1:]))),synthesized))\n",
    "x_test_deep = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "input_window = Input(shape=(window_length,))\n",
    "\n",
    "x = Dense(6, activation='relu')(input_window)\n",
    "x = BatchNormalization()(x)\n",
    "encoded = Dense(encoding_dim, activation='relu')(x)\n",
    "x = Dense(6, activation='relu')(encoded)\n",
    "x = BatchNormalization()(x)\n",
    "decoded = Dense(window_length, activation='sigmoid')(x)\n",
    "autoencoder = Model(input_window, decoded)\n",
    "\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = autoencoder.fit(x_train_deep, x_train_deep,\n",
    "                epochs=epochs,\n",
    "                batch_size=1024,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_deep, x_test_deep))\n",
    "\n",
    "decoded_stocks = autoencoder.predict(x_test_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_examples(x_test_simple, decoded_stocks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
